#  MADDPG implementation in PyTorch of Collaboration and Competition

## Introduction

This implementation is based on a straight-forward Multi-Agent implementation of DDPG. We have two agents each with its own actor and critic (including the target networks for each) where each critic receives the combined state as well as combined actions.

## Implementation details

The modules are divided as follows:
 - `engine.py` contains the main Multi-Agent code including the creation of each individual agent actors and critics (online & target)
 - `agent.py` defines the basic behavior of individual agents without the actual training procedure
 - `brain.py` defines the graphs that are used for actors and critics
 - `ma_per.py` contains the replay buffer definition including the prioritized experience buffers (_work in progress_)
 - `OUNoise.py` contains the Ornsteinâ€“Uhlenbeck process noise generation class
 - `main.py` is the local version of the main code that can be used locally (without the Jupyter Notebook)
 



## Future improvements

Work is already underway for a implementation of a prioritized experience buffer that will be based/updated using the TD-error generated by the individual critics (we sample separate batches of experiences for each individual agent).
A second improvement will be the addition of second critic for each agent in order to implement the TD3 approach.